# -*- coding: utf-8 -*-
"""Building a Chatbot LangChain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GA_2PGD1-ZZIkb9lzsaY4bQzRALXMzQA
"""

# 1) Install Ollama in the Colab VM
!curl -fsSL https://ollama.com/install.sh | sh

# 2) Start the server in the background
import subprocess, time
server = subprocess.Popen(["ollama", "serve"])
time.sleep(5)  # give it a moment to boot

# 3) Pull a smallish model
!ollama pull llama3.1

"""Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with Langsmith"""

!pip install langchain-ollama

from langchain_ollama import ChatOllama
llm = ChatOllama(base_url="http://127.0.0.1:11434", model="llama3.1", temperature=0.3)

from langchain.chat_models import init_chat_model

model = init_chat_model(
    "llama3.1",                # any model you've pulled in Ollama
    model_provider="ollama",
    base_url="http://127.0.0.1:11434",
    temperature=0.3,
)

"""Let's first use the model directly. `ChatModels` are instances of LangChain `"Runnables"`, which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the .invoke method.

The `.invoke()` method is the core execution method for calling a chain, model, or tool with some input and directly getting the output.
"""

from langchain_core.messages import HumanMessage
model.invoke([HumanMessage(content="Hi! I'm Bob")])

"""The model does not have the concept of state.Lets ask a followup question"""

model.invoke([HumanMessage(content="What's my name?")])

"""We need to pass the entire conversation history into the model to see what happens."""

from langchain_core.messages import AIMessage

model.invoke(
    [
        HumanMessage(content="Hi! I'm Bob"),
        AIMessage(content="Hello Bob! How can I assist you today?"),
        HumanMessage(content="What's my name?"),
    ]
)

"""# Message Persistence Layer

LangGraph establishes a implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.

Wrapping our chat model, in a minimal LangGraph application allows us to persist in the message history as it simplifies the development of multi-turn applications.

LangGraph comes with a simple in-memory checkpointer, which we use below.
"""

!pip install -U langgraph

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

# Define a new graph
# here the state in the graph has a message field
workflow = StateGraph(state_schema=MessagesState)


# Define the function that calls the model
# This is a node function: it takes the current graph state and does something.
# The state is updated with the model's reply
def call_model(state: MessagesState):
    response = model.invoke(state["messages"])   # extracts the current list of messages from the state.
    return {"messages": response}   # calls the LLM with the current conversation.


# Define the (single) node in the graph
workflow.add_edge(START, "model")    # connects the START node to the model
workflow.add_node("model", call_model) # creates a node called "model" that runs the function.

# Add memory
memory = MemorySaver() # used as a checkpoint that saves graph's intermediate state after a run
app = workflow.compile(checkpointer=memory)  # compile the graph with state remembered after invocations

"""Create a `config` that we pass into the runnable every time. The contains info not part of the input but useful."""

config = {"configurable": {"thread_id": "abc123"}}

"""This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.

We can then invoke the application:
"""

query = "Hi! I'm Bob."

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()  # output contains all messages in state

query = "What's my name?"

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()

"""Great, the chatbot remembers things now, now changing the config to a different thread reference now, hence we see a fresh conversation started"""

config = {"configurable": {"thread_id": "abc234"}}

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()

"""However, we can always go back to the original conversation (since we are persisting it in a database)"""

config = {"configurable": {"thread_id": "abc123"}}

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()

"""# Prompt Templates

It helps raw user iformtion into a format LLM can work with.  In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. Add system message custom instructions (with messages taking as input). Will add more input besides just the messages.

Lets create a `ChatPromptTemplate` where we will utilize `MessagePlaceholder` to pass all the messages in.
"""

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You talk like a Shashi Tharoor. Answer all questions to the best of your ability.",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

"""We will update our application to incorporte the given template."""

workflow = StateGraph(state_schema=MessagesState)

def call_model(state: MessagesState):
    prompt = prompt_template.invoke(state) # the prompt template added here which becomes a state
    response = model.invoke(prompt)
    return {"messages": response}

workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# Invoking the applications in the same way
config = {"configurable": {"thread_id": "abc345"}}
query = "Hi! I'm Jim."

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)   # add the prompt template within the same conversation ID
output["messages"][-1].pretty_print()

query = "What is my name?"

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()

"""Lets make it a bit more complicated. Lets assume that the prompt template now looks something like this with `language`  as the input to the prompt"""

prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. Answer all questions to the best of your ability in {language}.",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

from typing import Sequence

from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict


class State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    language: str


workflow = StateGraph(state_schema=State)


def call_model(state: State):
    prompt = prompt_template.invoke(state)
    response = model.invoke(prompt)
    return {"messages": [response]}


workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

config = {"configurable": {"thread_id": "abc456"}}
query = "Hi! I'm Deepak."
language = "Hindi"

input_messages = [HumanMessage(query)]
output = app.invoke(
    {"messages": input_messages, "language": language},
    config,
)
output["messages"][-1].pretty_print()

"""As the entire state persisted, so we omit parameters like `language` if no changes are desired."""

query = "What is my name?"

input_messages = [HumanMessage(query)]
output = app.invoke(
    {"messages": input_messages},
    config,
)
output["messages"][-1].pretty_print()

"""# Managing Conversation History

Important point is to manage the conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages that are being passed.

This can be done by adding a simple step in front of the prompt that modifies the `messages` key appropriately, and then wrap the new chain in the Message history class.

Langchain comes with a few in-built helpers to help that. The **trimmer** allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:
"""

from langchain_core.messages import SystemMessage, trim_messages

trimmer = trim_messages(
    max_tokens=65,
    strategy="last",
    token_counter=model,
    include_system=True,
    allow_partial=False,
    start_on="human",
)

messages = [
    SystemMessage(content="you're a good assistant"),
    HumanMessage(content="hi! I'm bob"),
    AIMessage(content="hi!"),
    HumanMessage(content="I like vanilla ice cream"),
    AIMessage(content="nice"),
    HumanMessage(content="whats 2 + 2"),
    AIMessage(content="4"),
    HumanMessage(content="thanks"),
    AIMessage(content="no problem!"),
    HumanMessage(content="having fun?"),
    AIMessage(content="yes!"),
]

trimmer.invoke(messages)

workflow = StateGraph(state_schema=State)

def call_model(state: State):
  print(f"Messages before trimming: {len(state['messages'])}")
  trimmed_messages = trimmer.invoke(state["messages"])
  print(f"Messages after trimming: {len(trimmed_messages)}")
  print("Remaining messages:")
  for msg in trimmed_messages:
        print(f"  {type(msg).__name__}: {msg.content}")
  prompt = prompt_template.invoke(
        {"messages": trimmed_messages, "language": state["language"]}
    )
  response = model.invoke(prompt)

  return {"messages": [response]}

workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

"""Now if we try asking the model our name, it will not know since we trimmed that part of the chat history (by defining the trim strategy as `last` the most recent messages are kept that fits within `max_token`)

"""

config = {"configurable": {"thread_id": "abc567"}}
query = "What is my name?"
language = "English"

input_messages = messages + [HumanMessage(query)]
output = app.invoke(
    {"messages": input_messages, "language": language},
    config,
)
output["messages"][-1].pretty_print()

"""But if we ask about information that is within the last few messages, it remembers."""

config = {"configurable": {"thread_id": "abc678"}}

query = "What math problem was asked?"
language = "English"

input_messages = messages + [HumanMessage(query)]
output = app.invoke(
    {"messages": input_messages, "language": language},
    config,
)
output["messages"][-1].pretty_print()

"""# Streaming

Now we've got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.

It's actually super easy to do this!


`.stream` in the LangGraph application streams application steps -- in this case, single step of the model response. The setting `stream_mode="messages"` allows us to stream the output tokens instead.

"""

config = {"configurable": {"thread_id": "abc789"}}
query = "Hi I'm Todd, please tell me a joke."
language = "English"

input_messages = [HumanMessage(query)]
for chunk, metadata in app.stream(
    {"messages": input_messages, "language": language},
    config,
    stream_mode="messages",
):
    if isinstance(chunk, AIMessage):  # Filter to just model responses
        print(chunk.content, end="|")

import ipywidgets as widgets
widgets.Widget.widget_types = {}